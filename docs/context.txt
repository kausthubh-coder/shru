# Studi — Context

## One-liner
**Learn faster. Prove skills.**  
(Currently delivering the **learn faster** part; proof/credentials come later.)

---

## Purpose
Replace one-size-fits-all college models and transform hiring by making learning personal, interactive, and verifiable over time.

---

## Mission (Two Parts)
1) **Learning (Studi App — live now in beta focus):**  
A realtime, voice-first AI tutor that adapts minute-by-minute and collaborates with you on a shared whiteboard, inside an IDE, and on a lesson page. Hyper-personalized, hands-on, and context-aware.

2) **Hiring (Proof of Skill — planned, not in current build):**  
Portable skill records and employer-friendly verification built on top of learning artifacts. **Not shipped yet.**

---

## Problem
- College content is static and paced for the average learner.  
- Most tools stop at “completion,” not true mastery.  
- Hiring still over-indexes on degrees instead of demonstrated skill.

---

## Solution (Current Scope = Learning)
A **realtime, voice-first AI tutor** that sees what you’re working on and teaches by doing:
- **Hyper-personalization:** one learner at a time; adapts to goals, knowledge gaps, and recent activity.
- **Interactive cognition:** teaches through drawing, coding, running examples, and generating lesson content on the fly.
- **Context awareness:** aware of your canvas, code, and lesson state to keep guidance grounded and specific.

> Note: “Prove skills” is a **future layer**. Today we’re laser-focused on making the learning loop exceptional.

---

## Product (What’s in the app now)
- **Realtime voice agent** that explains, asks questions, and guides your next step.
- **Shared whiteboard** for diagrams, derivations, and visual reasoning.
- Whiteboard text: the agent can add standalone text anywhere or label shapes via tools (`agent_create_text`, `agent_label`).
- **IDE workspace** (starting with Python) for writing/running code with agent assistance.
- **Lesson page with YAML renderer**: the agent writes a structured **YAML** spec that renders:
  - Markdown explanations
  - Inline quizzes/checkpoints
  - Lightweight custom UI elements (HTML/JS snippets) for interactive widgets
  - Code blocks and runnable examples
- **Interactive notes** tied to what you’re doing, not static lecture dumps.
- **Safety guardrails** for destructive actions (confirmations/approvals).


## Who It’s For (Starting Wedge)
- Self-learners and professionals upskilling in **STEM**.
- College or highschools students who want to master topics faster than a course timeline.


## Differentiators
- **Hyper-personal, live instruction** (not static videos or fixed pacing).
- **Hands-on interactivity**, not just chat: draw, code, run, and revise in one flow.
- **Context-aware tutoring** across whiteboard, code, and lesson state.
- **(Planned)** Verifiable skill outputs for hiring—added later on top of the learning loop.

---

## Current Status (Today)
- Working demo with:
  - Shared whiteboard
  - IDE (Python first)
  - YAML-driven lesson page
  - Interactive notes
  - Realtime voice agent that can guide, draw, and help debug
  - Basic confirmations for sensitive/destructive actions
- Pre-launch with a **waitlist**; shipping a beta to early users next.

---

## Near-Term Roadmap (Subject to Change)
- Public beta with early cohorts; tight feedback loop.
- Multi-language code execution; safer server-side runs.
- Persistence for files, runs, notes; lightweight learning analytics.
- Harden auth, rate limits, reliability, and latency.
- Early team/bootcamp use cases; pilot cohorts.
- Prep for future **skill transcripts/portfolios** (design now, implement later).

> Explicitly **out of scope for now**: employer credentials, external verifications, or any hiring integrations that imply completed “proof of skill.” Those will come **after** we nail the learning experience.

---

## Metrics We Care About (Learning)
- Time-to-skill: time-to-solve representative problems.
- Hint rate & hint decay: fewer/more targeted hints over time.
- Quiz/checkpoint outcomes tied to the current lesson state.
- Retention/return sessions and progression across topics.

---

## Risks & Mitigations
- **LLM reliability / hallucination** → Ground responses in user context (whiteboard/code/lesson), show steps, add runnable checks.
- **Latency** → Cache hot paths, stream partials, minimize round trips, selective tool calls.
- **Scope creep to hiring** → Keep shipping cadence focused on learning loop; design future verifications, but don’t ship them yet.

---

## Team & What We Need
- Building now; actively seeking early users (students, bootcamps, self-learners) for feedback.
- Looking for collaborators on:
  - Runtime safety & sandboxing
  - Educational design for interactive lessons
  - Future credentialing standards (design phase only)

---

## Call to Action
- **Join the waitlist** and **book a demo** to try the realtime tutor and YAML-driven lesson pages.
- Bring a problem you’re learning—let’s measure **time-to-skill** together.

---

## Auto‑context (technical)

What the model receives before most responses (sent over the realtime transport):

- A compact JSON block named `view_context` describing the current viewport
- A viewport‑bounded screenshot (data URL JPEG) when shapes are visible

view_context JSON shape:

```
{
  "type": "view_context",
  "bounds": { /* viewport box */ },
  "blurryShapes": [
    { _type, shapeId, x, y, w, h, text?, geo?, color?, fill? },
    // …up to ~60 items
  ],
  "peripheralClusters": [],
  "selectedShapes": [ { _type, shapeId, x, y, w, h, text?, geo? } ]
}
```

Send strategy:

- Combined message: JSON (as `input_text`) + image (as `input_image`) in one `conversation.item.create`
- Dedup window: ~300ms (skip resending if both JSON and image unchanged)
- Response trigger: wait ~120ms after sending context, then `response.create`
- Fallback: if combined send fails, send JSON first, then image (legacy path)

Transport events used:

- `conversation.item.create` — user message with `input_text` and optionally `input_image`
- `response.create` — instruct the server to generate a response

Notes:

- If no shapes are visible, the screenshot is omitted
- Errors are logged in the UI Logs panel and developer console
- Debug overlays (“Show Context”, “Show Calls”) mirror exactly what was sent
